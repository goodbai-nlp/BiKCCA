# BiKCCA
Improving Vector Space Word Representations Via Kernel Canonical Correlation Analysis
## To be solved


- [ ] [p0](However, this paper only compares the proposed model with its counterpart in Table 2 and Table 3. Actually, more comparison with other state-of-the-art modes should be presented.)

- [ ] [p1](However, the innovation of this paper is limited)

- [ ] [p2](But what is non-linear relationship between languages ?)

- [ ] [p3](Can we see examples illustrating this relationship ?)

- [x] [p4](And why BiKCCA is suitable to deal with such relationship ?)

- [ ] [p5](There is no Related Work section in the paper to give readers a clear overview about the task and the background, which also should be able to help readers understand what is the novelty of the proposed model compared to previous works in the literature.)

- [ ] [p6](In section 4, the authors discuss 3 tasks to evaluate the quality of the induced cross-lingual word embeddings with the corresponding datasets. Again, in section 5.1, the  datasets are explained again with slightly more detail. Please organize these two parts more focus and briefly. It would be nice to see more discussion about the differences between datasets.)

- [ ] [p7](In Table 3, En-L2, why BiKCCA have improvements over BiCCA in all the language except French? The authors should provide explanations.)

- [ ] [p8](In Table 4, how about the scores of (overly, 过于) and (notice, 注意) ？)

- [ ] [p9](Then, what are the state-of-art results in the monolingual word similarity task , cross-lingual dictionary induction task and the cross-lingual document classification task? The authors should compare the proposed model with state-of-the-art methods, on several benchmark datasets.  For example, are there any similar methods like BiKCCA? Or any comparable methods performed in the three tasks, such as introducing other resource to enrich the embeddings? You have to compare with other works to validate the proposed method.)
